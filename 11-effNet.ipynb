{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apex.fp16_utils as fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastprogress import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from exp.nb_10c import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#effNets are built with varying coefficients...use effNet_Type[0...7] to load params\n",
    "\n",
    "#enet type / width expansion / depth expansion / dropout rate / resolution\n",
    "effNet_Type = [\n",
    "    [ 0, 1.0, 1.0, 0.2, 224],\n",
    "    [ 1, 1.0, 1.1, 0.2, 240],\n",
    "    [ 2, 1.1, 1.2, 0.3, 260],\n",
    "    [ 3, 1.2, 1.4, 0.3, 300],\n",
    "    [ 4, 1.4, 1.8, 0.4, 380],\n",
    "    [ 5, 1.6, 2.2, 0.4, 456],\n",
    "    [ 6, 1.8, 2.6, 0.5, 528],\n",
    "    [ 7, 2.0, 3.1, 0.5, 600],\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1.0, 1.0, 0.2]\n"
     ]
    }
   ],
   "source": [
    "print(effNet_params[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x = x * torch.sigmoid(x)  #nn.functional.sigmoid is deprecated, use torch.sigmoid instead\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x): \n",
    "        return x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_cnn(m):\n",
    "    \"\"\"init cnn with kaiming weights.  Recurses through model layer by layer\"\"\"\n",
    "    if getattr(m,'bias',None) is not None:\n",
    "        nn.init.constant_(m.bias,0)\n",
    "    if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.kaiming_uniform_(m.weight)\n",
    "        \n",
    "    for l in m.children():\n",
    "        init_cnn(l)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def econv(ni, nf, ks=3, stride=1, groups=1, bias=False):\n",
    "    return nn.Conv2d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2, groups= groups, bias=bias)\n",
    "\n",
    "\n",
    "def econv_layer(ni, nf, ks=3, stride=1, groups=1, zero_bn=False, act=True, eps=1e-03, momentum=0.01):\n",
    "    \n",
    "    bn = nn.BatchNorm2d(nf, eps=eps, momentum=momentum)\n",
    "    nn.init.constant_(bn.weight, 0. if zero_bn else 1.)\n",
    "    \n",
    "    layers = [econv(ni, nf, ks, stride=stride, groups=groups), bn]\n",
    "    \n",
    "    if act: \n",
    "        layers.append(act_fn)\n",
    "        \n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop connect.  Two implementations, use second one due to fp16 training issue per Seb\n",
    "# not compatible with fp16 training  \n",
    "\n",
    "class Drop_Connect(nn.Module):\n",
    "    \"\"\"create a tensor mask and apply to inputs, for removing drop_ratio % of weights\"\"\"\n",
    "    def __init__(self, drop_ratio=0):\n",
    "        super().__init__()\n",
    "        self.keep_percent = 1.0 - drop_ratio\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.training:\n",
    "            return x\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        random_tensor = self.keep_percent\n",
    "        random_tensor += torch.rand([batch_size, 1, 1, 1], dtype=x.dtype,device=x.device)   #dtype is causing issues with fp16 training\n",
    "        binary_tensor = torch.floor(random_tensor)\n",
    "        output = x / self.keep_percent * binary_tensor\n",
    "\n",
    "        return output\n",
    "    \n",
    "    \n",
    "def edrop_connect(inputs, p, training):\n",
    "    \"\"\" Drop connect. \"\"\"\n",
    "    if not training: return inputs\n",
    "    batch_size = inputs.shape[0]\n",
    "    keep_prob = 1 - p\n",
    "    random_tensor = keep_prob\n",
    "    random_tensor += torch.rand([batch_size, 1, 1, 1], dtype=inputs.dtype,device=inputs.device)  # uniform [0,1)\n",
    "    binary_tensor = torch.floor(random_tensor)\n",
    "    output = inputs / keep_prob * binary_tensor\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#squeeze and excite block:\n",
    "class eSqueeze(nn.Module):\n",
    "    def __init__(self, ni, reduce_ratio=.25):\n",
    "        super().__init__()\n",
    "        \n",
    "        reduced_channels = max(1, int(ni * reduce_ratio))\n",
    "        #print(\"reduced = \",reduced_channels)\n",
    "        \n",
    "        layers = [nn.AdaptiveAvgPool2d(1),\n",
    "                      econv(ni, reduced_channels, ks=1, bias=True),  # in TF code, padding = 'same', ?? should be zero here\n",
    "                      act_fn,\n",
    "                      econv(reduced_channels, ni, ks=1, bias=True),\n",
    "                      nn.Sigmoid()]\n",
    "        \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return x * self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class eMBConvBlock(nn.Module):\n",
    "    def __init__(self, ni, nf, expansion=1, \n",
    "                 ks=3, stride=2, skip=True,\n",
    "                 squeeze_ratio=.25, drop_connect_ratio=.2):\n",
    "        super().__init__()\n",
    "\n",
    "        nh = ni * expansion  #how much expansion from input count to middle/hidden count\n",
    "\n",
    "        #1st layer, expansion\n",
    "        if expansion !=1:\n",
    "            self.expansion = econv_layer(ni, nh, ks=1, bias=False)\n",
    "        else:\n",
    "            self.expansion = nn.Identity()  #identity=no-op, forward(x)\n",
    "\n",
    "        #2nd layer, depthwise conv\n",
    "        self.depthwise = econv_layer(nh, nh, ks=ks, stride=stride, groups=nh, bias=False)\n",
    "    \n",
    "\n",
    "        #3rd layer\n",
    "        self.sqex = eSqueeze(nh, squeeze_ratio) if squeeze_ratio >0 else nn.Identity()\n",
    "\n",
    "        #4th layer\n",
    "        self.projection = econv_layer(nh, nf, ks=1, stride=1, bias=False)\n",
    "        \n",
    "\n",
    "        self.skip = skip and (stride==1) and (ni==nf)\n",
    "        if self.skip:\n",
    "            self.dropconnect = partial(drop_connect(p=drop_connect_ratio, training=self.training))\n",
    "        else:\n",
    "            self.dropconnect=nn.Identity()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        expand = self.expansion(inputs)\n",
    "        dwise = self.depthwise(expand)\n",
    "        se = self.sqex(dwise)\n",
    "        x = self.projection(se)\n",
    "        if self.skip:\n",
    "            x = x+ self.dropconnect(inputs)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_filters(filters, depth_multi, divisor=8, min_depth=None):\n",
    "    \n",
    "    \"\"\"Round number of filters based on depth multiplier.\n",
    "    see: https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/efficientnet_model.py\"\"\"\n",
    "    \n",
    "    orig_f = filters\n",
    "    \n",
    "    if not depth_multi:\n",
    "        return filters\n",
    "\n",
    "    filters = [f*depth_multi for f in filters]\n",
    "    min_depth = min_depth or divisor\n",
    "    new_filters = [max(min_depth, int(f + divisor / 2) // divisor * divisor) for f in filters]\n",
    "    # prevent rounding by more than 10%\n",
    "    new_filters = [new_filters[i] + (new_filters[i] < 0.9 * filters[i])* divisor for i in range(len(new_filters))]\n",
    "    new_filters = [int(f) for f in new_filters]\n",
    "    #print('round_filter input={} output={}'.format(orig_f, new_filters))\n",
    "    return new_filters\n",
    "\n",
    "\n",
    "def round_repeats(repeats, global_params):\n",
    "    \n",
    "    \"\"\"Round number of filters based on depth multiplier.\"\"\"\n",
    "    multiplier = global_params.depth_coefficient\n",
    "    if not multiplier:\n",
    "        return repeats\n",
    "    return int(math.ceil(multiplier * repeats))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class effNet(nn.Sequential):\n",
    "    def __init__(self, channels, repeat, \n",
    "                 ks, stride, expand, width_multi=1.0, depth_multi=1.0, \n",
    "                 se = None, drop_connect_rate = None,dropout_rate= None, \n",
    "                 c_in=3, c_out=10):\n",
    "\n",
    "        \n",
    "        repeat = [int(math.ceil(r*depth_multi)) for r in repeat]\n",
    "        channels = round_filters(channels, width_multi)\n",
    "        \n",
    "        stem = [econv_layer(c_in, channels[0], ks=3 ,stride=2)]\n",
    "\n",
    "        blocks = []\n",
    "        #The first block needs to take care of stride and filter size increase.\n",
    "\n",
    "        for i in range(len(repeat)):\n",
    "            blocks+= [eMBConvBlock(channels[i], channels[i+1], expand[i], ks=ks[i], \n",
    "                                   stride=stride[i], se = se, drop_connect_rate=drop_connect_rate)]\n",
    "            \n",
    "            blocks+= [eMBConvBlock(channels[i+1], channels[i+1], expand[i], ks=ks[i], \n",
    "                                   stride=1, se = se, drop_connect_rate=drop_connect_rate)] *(repeat[i]-1)\n",
    "\n",
    "        dropout = nn.Dropout(p=dropout_rate) if dropout_rate else nn.Identity()\n",
    "\n",
    "        head = [conv_layer(channels[-2], channels[-1], ks=1 ,stride=1), \n",
    "                nn.AdaptiveAvgPool2d(1), Flatten(), dropout, \n",
    "                nn.Linear(channels[-1], c_out)]\n",
    "\n",
    "\n",
    "        super().__init__(*stem, *blocks, *head)\n",
    "                      \n",
    "        init_cnn(self)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "me = sys.modules[__name__]\n",
    "channels = [32,16,24,40,80,112,192,320,1280]  #9 count\n",
    "repeat = [1,2,2,3,3,4,1]\n",
    "ks = [3,3,5,3,5,5,3]\n",
    "stride = [1,2,2,2,1,2,1]\n",
    "exp = [1,6,6,6,6,6,6]\n",
    "se = 0.25\n",
    "do = 0.2\n",
    "dc=0.2\n",
    "\n",
    "\n",
    "# base without multipliers and dropout\n",
    "setattr(me, 'effnet', partial(effNet, channels=channels, repeat=repeat, ks=ks, stride=stride, \n",
    "                                    expand=exp, se=se, drop_connect_rate=dc))\n",
    "\n",
    "# (number, width_coefficient, depth_coefficient, dropout_rate) \n",
    "for n, wm, dm, do in [\n",
    "    [ 0, 1.0, 1.0, 0.2],\n",
    "    [ 1, 1.0, 1.1, 0.2],\n",
    "    [ 2, 1.1, 1.2, 0.3],\n",
    "    [ 3, 1.2, 1.4, 0.3],\n",
    "    [ 4, 1.4, 1.8, 0.4],\n",
    "    [ 5, 1.6, 2.2, 0.4],\n",
    "    [ 6, 1.8, 2.6, 0.5],\n",
    "    [ 7, 2.0, 3.1, 0.5],\n",
    "]:\n",
    "    name = f'effNetB{n}'\n",
    "    setattr(me, name, partial(effnet, depth_multi=dm, width_multi=wm, dropout_rate=do))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imagenet(te) training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightRelu(nn.Module):\n",
    "    #.46 was found to shift the mean to 0 on a random distribution test\n",
    "    # maxv of 7.5 was from initial testing on MNIST.  \n",
    "    #Important - cut your learning rates in half with this...\n",
    "    \n",
    "    def __init__(self,sub=.2,maxv=None):\n",
    "        super().__init__()\n",
    "        self.sub=sub\n",
    "        self.maxv=maxv\n",
    "    \n",
    "    def forward(self,x):\n",
    "        #change to lisht\n",
    "        \n",
    "        x = x *torch.tanh(x)\n",
    "        \n",
    "        if self.sub is not None:\n",
    "            x.sub_(self.sub)\n",
    "        if self.maxv is not None: \n",
    "            x.clamp_max_(self.maxv)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = datasets.untar_data(datasets.URLs.IMAGENETTE_160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 128\n",
    "tfms = [make_rgb, RandomResizedCrop(128,scale=(0.35,1)), np_to_float, PilRandomFlip()]\n",
    "\n",
    "bs = 24\n",
    "\n",
    "il = ImageList.from_files(path, tfms=tfms)\n",
    "sd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name='val'))\n",
    "ll = label_by_func(sd, parent_labeler, proc_y=CategoryProcessor())\n",
    "\n",
    "ll.valid.x.tfms = [make_rgb, CenterCrop(size), np_to_float]\n",
    "\n",
    "data = ll.to_databunch(bs, c_in=3, c_out=10, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def noop(x): return x\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x): return x.view(x.size(0), -1)\n",
    "\n",
    "def conv(ni, nf, ks=3, stride=1, bias=False):\n",
    "    return nn.Conv2d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2, bias=bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noop(x): return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self,x): return x.view(x.size(0),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#act_fn = LightRelu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralRelu(nn.Module):\n",
    "    def __init__(self, leak=.2, sub=.3, maxv=12):\n",
    "        super().__init__()\n",
    "        self.leak,self.sub,self.maxv = leak,sub,maxv\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = F.leaky_relu(x,self.leak) if self.leak is not None else F.relu(x)\n",
    "        if self.sub is not None: x.sub_(self.sub)\n",
    "        if self.maxv is not None: x.clamp_max_(self.maxv)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FTSwish2(nn.Module):\n",
    "    def __init__(self, threshold=-.25):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold #,self.sub,self.maxv = threshold,sub,maxv\n",
    "\n",
    "    def forward(self, x): \n",
    "        print(x)\n",
    "        #if x > 0:\n",
    "        x = (x*torch.sigmoid(x)) + threshold\n",
    "       # else:\n",
    "       #     x = threshold\n",
    "            \n",
    "        #if self.sub is not None: x.sub_(self.sub)\n",
    "       # if self.maxv is not None: x.clamp_max_(self.maxv)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#act_fn = LightRelu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#act_fn = nn.ReLU(inplace=True)\n",
    "\n",
    "def init_cnn(m):\n",
    "    if getattr(m, 'bias', None) is not None: nn.init.constant_(m.bias, 0)\n",
    "    if isinstance(m, (nn.Conv2d,nn.Linear)): nn.init.kaiming_normal_(m.weight)\n",
    "    for l in m.children(): init_cnn(l)\n",
    "\n",
    "def conv_layer(ni, nf, ks=3, stride=1, zero_bn=False, act=True):\n",
    "    bn = nn.BatchNorm2d(nf)\n",
    "    nn.init.constant_(bn.weight, 0. if zero_bn else 1.)\n",
    "    layers = [conv(ni, nf, ks, stride=stride), bn]\n",
    "    if act: layers.append(act_fn)\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#act_fn = nn.ReLU(inplace=True)\n",
    "\n",
    "def init_cnn2(m):\n",
    "    if getattr(m, 'bias',None) is not None:  nn.init.constant_(m.bias,0)\n",
    "    if isinstance(m, (nn.Conv2d, nn.Linear)): nn.init.kaiming_normal_(m.weight)\n",
    "    for l in m.children(): init_cnn(l)\n",
    "        \n",
    "def conv_layer2(ni, nf, ks=3, stride = 1, zero_bn=False, act=True):\n",
    "    bn = nn.BatchNorm2d(nf)\n",
    "    nn.init.constant_(bn.weight,0. if zero_bn else 1.)\n",
    "    layers = [conv(ni,nf,ks,stride=stride),bn]\n",
    "    if act: layers.append(act_fn)\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, expansion, ni, nh, stride=1):\n",
    "        super().__init__()\n",
    "        nf,ni = nh*expansion,ni*expansion\n",
    "        layers  = [conv_layer(ni, nh, 1)]\n",
    "        layers += [\n",
    "            conv_layer(nh, nf, 3, stride=stride, zero_bn=True, act=False)\n",
    "        ] if expansion==1 else [\n",
    "            conv_layer(nh, nh, 3, stride=stride),\n",
    "            conv_layer(nh, nf, 1, zero_bn=True, act=False)\n",
    "        ]\n",
    "        self.convs = nn.Sequential(*layers)\n",
    "        self.idconv = noop if ni==nf else conv_layer(ni, nf, 1, act=False)\n",
    "        self.pool = noop if stride==1 else nn.AvgPool2d(2)\n",
    "\n",
    "    def forward(self, x): return act_fn(self.convs(x) + self.idconv(self.pool(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock2(nn.Module):\n",
    "    def __init__(self, expansion, ni,nh, stride=1):\n",
    "        super().__init__()\n",
    "        nf, ni = nh*expansion, ni*expansion  #number of filters/fields, number of inputs\n",
    "        layers = [conv_layer(ni,nh,1)]  #base layer\n",
    "        layers += [\n",
    "            conv_layer(nh,nh,3,stride=stride, zero_bn=True, act=False) #add new conv layer if expansion =1 else\n",
    "        ] if expansion==1 else [\n",
    "            conv_layer(nh,nh,3,stride=stride),\n",
    "            conv_layer(nh,nf,1, zero_bn=True, act=False) #add two conv layerss\n",
    "        ]\n",
    "        self.convs = nn.Sequential(*layers)  #wrap it in a sequential\n",
    "        self.idconv = noop if ni==nf else conv_layer(ni,nf,1,act=False)  # add id layer\n",
    "        self.pool = noop if stride==1 else nn.AvgPool2d(2) # add pool layer\n",
    "        \n",
    "    def forward(self,x): return act_fn(self.convs(x)+ self.idconv(self.pool(x))) #wrap block in relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class XResNet(nn.Sequential):\n",
    "    @classmethod\n",
    "    def create(cls, expansion, layers, c_in=3, c_out=1000):\n",
    "        nfs = [c_in, (c_in+1)*8, 64, 64]\n",
    "        stem = [conv_layer(nfs[i], nfs[i+1], stride=2 if i==0 else 1)\n",
    "            for i in range(3)]\n",
    "\n",
    "        nfs = [64//expansion,64,128,256,512]\n",
    "        res_layers = [cls._make_layer(expansion, nfs[i], nfs[i+1],\n",
    "                                      n_blocks=l, stride=1 if i==0 else 2)\n",
    "                  for i,l in enumerate(layers)]\n",
    "        res = cls(\n",
    "            *stem,\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            *res_layers,\n",
    "            nn.AdaptiveAvgPool2d(1), Flatten(),\n",
    "            nn.Linear(nfs[-1]*expansion, c_out),\n",
    "        )\n",
    "        init_cnn(res)\n",
    "        return res\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_layer(expansion, ni, nf, n_blocks, stride):\n",
    "        return nn.Sequential(\n",
    "            *[ResBlock(expansion, ni if i==0 else nf, nf, stride if i==0 else 1)\n",
    "              for i in range(n_blocks)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def xresnet18 (**kwargs): return XResNet.create(1, [2, 2, 2, 2], **kwargs)\n",
    "def xresnet34 (**kwargs): return XResNet.create(1, [3, 4, 6, 3], **kwargs)\n",
    "def xresnet50 (**kwargs): return XResNet.create(4, [3, 4, 6, 3], **kwargs)\n",
    "def xresnet101(**kwargs): return XResNet.create(4, [3, 4, 23, 3], **kwargs)\n",
    "def xresnet152(**kwargs): return XResNet.create(4, [3, 8, 36, 3], **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbfs = [partial(AvgStatsCallback,accuracy), ProgressCallback, CudaCallback,\n",
    "        partial(BatchTransformXCallback, norm_imagenette),\n",
    "#         partial(MixUp, alpha=0.2)\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = LabelSmoothingCrossEntropy()\n",
    "arch = partial(xresnet18, c_out=10)\n",
    "opt_func = adam_opt(mom=0.9, mom_sqr=0.99, eps=1e-6, wd=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_batch(dl, learn):\n",
    "    learn.xb,learn.yb = next(iter(dl))\n",
    "    learn.do_begin_fit(0)\n",
    "    learn('begin_batch')\n",
    "    learn('after_fit')\n",
    "    return learn.xb,learn.yb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to replace the old `model_summary` since it used to take a `Runner`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def model_summary(model, find_all=False, print_mod=False):\n",
    "    xb,yb = get_batch(data.valid_dl, learn)\n",
    "    mods = find_modules(model, is_lin_layer) if find_all else model.children()\n",
    "    f = lambda hook,mod,inp,out: print(f\"====\\n{mod}\\n\" if print_mod else \"\", out.shape)\n",
    "    with Hooks(mods, f) as hooks: learn.model(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn = Learner(arch(), data, loss_func, lr=1, cb_funcs=cbfs, opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.model = learn.model.cuda()\n",
    "#model_summary(learn.model, print_mod=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arch = partial(xresnet50, c_out=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn = Learner(arch(), data, loss_func, lr=1, cb_funcs=cbfs, opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.fit(1, cbs=[LR_Find(), Recorder()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.recorder.plot(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.recorder.plot_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_phases(phases):\n",
    "    phases = listify(phases)\n",
    "    return phases + [1-sum(phases)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3, 0.7]\n",
      "[0.3, 0.2, 0.5]\n"
     ]
    }
   ],
   "source": [
    "print(create_phases(0.3))\n",
    "print(create_phases([0.3,0.2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "pct_start = 0.5\n",
    "phases = create_phases(pct_start)\n",
    "sched_lr  = combine_scheds(phases, cos_1cycle_anneal(lr/10., lr, lr/1e5))\n",
    "sched_mom = combine_scheds(phases, cos_1cycle_anneal(0.95,0.85, 0.95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-15\n"
     ]
    }
   ],
   "source": [
    "print(1e-15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbsched = [\n",
    "    ParamScheduler('lr', sched_lr),\n",
    "    ParamScheduler('mom', sched_mom)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn = Learner(arch(), data, loss_func, lr=lr, cb_funcs=cbfs, opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.fit(1, cbs=cbsched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cnn_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def cnn_learner(arch, data, loss_func, opt_func, c_in=None, c_out=None,\n",
    "                lr=1e-1, cuda=True, norm=None, progress=True, mixup=0, xtra_cb=None, **kwargs):\n",
    "    cbfs = [partial(AvgStatsCallback,accuracy)]+listify(xtra_cb)\n",
    "    if progress: cbfs.append(ProgressCallback)\n",
    "    if cuda:     cbfs.append(CudaCallback)\n",
    "    if norm:     cbfs.append(partial(BatchTransformXCallback, norm))\n",
    "    if mixup:    cbfs.append(partial(MixUp, mixup))\n",
    "    arch_args = {}\n",
    "    if not c_in : c_in  = data.c_in\n",
    "    if not c_out: c_out = data.c_out\n",
    "    if c_in:  arch_args['c_in' ]=c_in\n",
    "    if c_out: arch_args['c_out']=c_out\n",
    "    return Learner(arch(**arch_args), data, loss_func, opt_func=opt_func, lr=lr, cb_funcs=cbfs, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch.nn.functional as F  (uncomment if needed)\n",
    "\n",
    "class FTSwish(nn.Module):\n",
    "    def __init__(self, threshold=-.25, mean_shift=-.1):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "        self.mean_shift = mean_shift\n",
    "        #warning - does not handle multi-gpu case below\n",
    "        #self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "\n",
    "    def forward(self, x): \n",
    "        \n",
    "        x = F.relu(x) * torch.sigmoid(x) + self.threshold\n",
    "        \n",
    "        #note on above: (\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
    "        \n",
    "        \n",
    "        #FTSwish+ for positive values\n",
    "        #pos_value = (x*torch.sigmoid(x)) + self.threshold\n",
    "        \n",
    "        #FTSwish+ for negative values\n",
    "        #tval = torch.tensor([self.threshold],device=self.device)\n",
    "        \n",
    "        #apply to x tensor based on positive or negative value\n",
    "        #x = torch.where(x>=0, pos_value, tval)\n",
    "        \n",
    "        \n",
    "        #apply mean shift to drive mean to 0. -.1 was tested as optimal for kaiming init\n",
    "        if self.mean_shift is not None:\n",
    "            x.sub_(self.mean_shift)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch.nn.functional as F  (uncomment if needed,but you likely already have it)\n",
    "\n",
    "class FTSwishPlus(nn.Module):\n",
    "    def __init__(self, threshold=-.25, mean_shift=-.1):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "        self.mean_shift = mean_shift\n",
    "\n",
    "    def forward(self, x): \n",
    "        \n",
    "        x = F.relu(x) * torch.sigmoid(x) + self.threshold        \n",
    "        #note on above - why not F.sigmoid?: \n",
    "        #PyTorch docs - (\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
    "        \n",
    "        #apply mean shift to drive mean to 0. -.1 was tested as optimal for kaiming init\n",
    "        if self.mean_shift is not None:\n",
    "            x.sub_(self.mean_shift)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRelu(nn.Module):\n",
    "    def __init__(self, threshold= - .25, mean_shift=-.03):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "        self.mean_shift = mean_shift\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = F.relu(x)+self.threshold\n",
    "        \n",
    "        if self.mean_shift is not None:\n",
    "            x.sub_(self.mean_shift)\n",
    "            \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_fn = ReluT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(FTSwish.forward(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = cnn_learner(xresnet34, data, loss_func, opt_func, norm=norm_imagenette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.708927</td>\n",
       "      <td>0.475260</td>\n",
       "      <td>1.455793</td>\n",
       "      <td>0.612000</td>\n",
       "      <td>11:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.380989</td>\n",
       "      <td>0.635567</td>\n",
       "      <td>1.319252</td>\n",
       "      <td>0.676000</td>\n",
       "      <td>09:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.329379</td>\n",
       "      <td>0.660540</td>\n",
       "      <td>1.855722</td>\n",
       "      <td>0.558000</td>\n",
       "      <td>09:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.308054</td>\n",
       "      <td>0.666822</td>\n",
       "      <td>1.320109</td>\n",
       "      <td>0.674000</td>\n",
       "      <td>10:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.275966</td>\n",
       "      <td>0.684970</td>\n",
       "      <td>1.510024</td>\n",
       "      <td>0.626000</td>\n",
       "      <td>09:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.232606</td>\n",
       "      <td>0.706685</td>\n",
       "      <td>1.111155</td>\n",
       "      <td>0.756000</td>\n",
       "      <td>10:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.167539</td>\n",
       "      <td>0.733209</td>\n",
       "      <td>1.037894</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>11:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.093275</td>\n",
       "      <td>0.758803</td>\n",
       "      <td>1.066288</td>\n",
       "      <td>0.788000</td>\n",
       "      <td>12:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.015583</td>\n",
       "      <td>0.792772</td>\n",
       "      <td>0.855244</td>\n",
       "      <td>0.866000</td>\n",
       "      <td>09:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.935219</td>\n",
       "      <td>0.826198</td>\n",
       "      <td>0.846282</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>17:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.862420</td>\n",
       "      <td>0.861951</td>\n",
       "      <td>0.790140</td>\n",
       "      <td>0.896000</td>\n",
       "      <td>09:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.840959</td>\n",
       "      <td>0.868233</td>\n",
       "      <td>0.781657</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>14:12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(12, cbsched) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.856373</td>\n",
       "      <td>0.861176</td>\n",
       "      <td>0.874657</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>13:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.950700</td>\n",
       "      <td>0.820149</td>\n",
       "      <td>0.956492</td>\n",
       "      <td>0.824000</td>\n",
       "      <td>16:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.022369</td>\n",
       "      <td>0.792229</td>\n",
       "      <td>1.068875</td>\n",
       "      <td>0.768000</td>\n",
       "      <td>15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.942604</td>\n",
       "      <td>0.827749</td>\n",
       "      <td>0.840393</td>\n",
       "      <td>0.876000</td>\n",
       "      <td>07:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.836331</td>\n",
       "      <td>0.873740</td>\n",
       "      <td>0.750392</td>\n",
       "      <td>0.904000</td>\n",
       "      <td>05:29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(5,cbsched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imagenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see all this put together in the fastai [imagenet training script](https://github.com/fastai/fastai/blob/master/examples/train_imagenet.py). It's the same as what we've seen so far, except it also handles multi-GPU training. So how well does this work?\n",
    "\n",
    "We trained for 60 epochs, and got an error of 5.9%, compared to the official PyTorch resnet which gets 7.5% error in 90 epochs! Our xresnet 50 training even surpasses standard resnet 152, which trains for 50% more epochs and has 3x as many layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "##!./notebook2script.py 11_train_imagenette.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
