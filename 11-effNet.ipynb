{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apex.fp16_utils as fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastprogress import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from exp.nb_10c import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#effNets are built with varying coefficients...use effNet_Type[0...7] to load params\n",
    "\n",
    "#enet type / width expansion / depth expansion / dropout rate / resolution\n",
    "effNet_Type = [\n",
    "    [ 0, 1.0, 1.0, 0.2, 224],\n",
    "    [ 1, 1.0, 1.1, 0.2, 240],\n",
    "    [ 2, 1.1, 1.2, 0.3, 260],\n",
    "    [ 3, 1.2, 1.4, 0.3, 300],\n",
    "    [ 4, 1.4, 1.8, 0.4, 380],\n",
    "    [ 5, 1.6, 2.2, 0.4, 456],\n",
    "    [ 6, 1.8, 2.6, 0.5, 528],\n",
    "    [ 7, 2.0, 3.1, 0.5, 600],\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1.0, 1.0, 0.2]\n"
     ]
    }
   ],
   "source": [
    "print(effNet_params[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x = x * torch.sigmoid(x)  #nn.functional.sigmoid is deprecated, use torch.sigmoid instead\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x): \n",
    "        return x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_cnn(m):\n",
    "    \"\"\"init cnn with kaiming weights.  Recurses through model layer by layer\"\"\"\n",
    "    if getattr(m,'bias',None) is not None:\n",
    "        nn.init.constant_(m.bias,0)\n",
    "    if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.kaiming_uniform_(m.weight)\n",
    "        \n",
    "    for l in m.children():\n",
    "        init_cnn(l)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from:  https://github.com/lukemelas/EfficientNet-PyTorch/blob/master/efficientnet_pytorch/utils.py\n",
    "\n",
    "class Conv2dSamePadding(nn.Conv2d):\n",
    "    \"\"\" 2D Convolutions like TensorFlow \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, groups=1, bias=True):\n",
    "        super().__init__(in_channels, out_channels, kernel_size, stride, 0, dilation, groups, bias)\n",
    "        self.stride = self.stride if len(self.stride) == 2 else [self.stride[0]]*2\n",
    "\n",
    "    def forward(self, x):\n",
    "        ih, iw = x.size()[-2:]\n",
    "        kh, kw = self.weight.size()[-2:]\n",
    "        sh, sw = self.stride\n",
    "        oh, ow = math.ceil(ih / sh), math.ceil(iw / sw)\n",
    "        pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n",
    "        pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)\n",
    "        if pad_h > 0 or pad_w > 0:\n",
    "            x = F.pad(x, [pad_w//2, pad_w - pad_w//2, pad_h//2, pad_h - pad_h//2])\n",
    "        return F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def econv(ni, nf, ks=3, stride=1, groups=1, bias=False):\n",
    "    #return nn.Conv2d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2, groups= groups, bias=bias)\n",
    "    return Conv2dSamePadding(ni, nf, kernel_size=ks, stride=stride, groups= groups, bias=bias)\n",
    "\n",
    "\n",
    "def econv_layer(ni, nf, ks=3, stride=1, groups=1, zero_bn=False, bias=False, act=True, eps=1e-03, momentum=0.01):\n",
    "    \n",
    "    bn = nn.BatchNorm2d(nf, eps=eps, momentum=momentum)\n",
    "    nn.init.constant_(bn.weight, 0. if zero_bn else 1.)\n",
    "    \n",
    "    layers = [econv(ni, nf, ks, stride=stride, groups=groups), bn]\n",
    "    \n",
    "    if act: \n",
    "        layers.append(act_fn)\n",
    "        \n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop connect.  Two implementations, use second one due to fp16 training issue per Seb\n",
    "# not compatible with fp16 training  \n",
    "\n",
    "class Drop_Connect(nn.Module):\n",
    "    \"\"\"create a tensor mask and apply to inputs, for removing drop_ratio % of weights\"\"\"\n",
    "    def __init__(self, drop_ratio=0):\n",
    "        super().__init__()\n",
    "        self.keep_percent = 1.0 - drop_ratio\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.training:\n",
    "            return x\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        random_tensor = self.keep_percent\n",
    "        random_tensor += torch.rand([batch_size, 1, 1, 1], dtype=x.dtype,device=x.device)   #dtype is causing issues with fp16 training\n",
    "        binary_tensor = torch.floor(random_tensor)\n",
    "        output = x / self.keep_percent * binary_tensor\n",
    "\n",
    "        return output\n",
    "    \n",
    "    \n",
    "def edrop_connect(inputs, p, training):\n",
    "    \"\"\" Drop connect. \"\"\"\n",
    "    if not training: return inputs\n",
    "    batch_size = inputs.shape[0]\n",
    "    keep_prob = 1 - p\n",
    "    random_tensor = keep_prob\n",
    "    random_tensor += torch.rand([batch_size, 1, 1, 1], dtype=inputs.dtype,device=inputs.device)  # uniform [0,1)\n",
    "    binary_tensor = torch.floor(random_tensor)\n",
    "    output = inputs / keep_prob * binary_tensor\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#squeeze and excite block:\n",
    "class eSqueeze(nn.Module):\n",
    "    def __init__(self, ni, reduce_ratio=.25):\n",
    "        super().__init__()\n",
    "        \n",
    "        reduced_channels = max(1, int(ni * reduce_ratio))\n",
    "        #print(\"reduced = \",reduced_channels)\n",
    "        \n",
    "        layers = [nn.AdaptiveAvgPool2d(1),\n",
    "                      econv(ni, reduced_channels, ks=1, bias=True),  # in TF code, padding = 'same', ?? should be zero here\n",
    "                      act_fn,\n",
    "                      econv(reduced_channels, ni, ks=1, bias=True),\n",
    "                      nn.Sigmoid()]\n",
    "        \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return x * self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class eMBConvBlock(nn.Module):\n",
    "    def __init__(self, ni, nf, expansion=1, \n",
    "                 ks=3, stride=2, skip=True,\n",
    "                 squeeze_ratio=.25, drop_connect_prob=.2):\n",
    "        super().__init__()\n",
    "\n",
    "        nh = ni * expansion  #how much expansion from input count to middle/hidden count\n",
    "\n",
    "        #1st layer, expansion\n",
    "        if expansion !=1:\n",
    "            self.expansion = econv_layer(ni, nh, ks=1, bias=False)\n",
    "        else:\n",
    "            self.expansion = Identity  #identity=no-op, forward(x)\n",
    "\n",
    "        #2nd layer, depthwise conv\n",
    "        self.depthwise = econv_layer(nh, nh, ks=ks, stride=stride, groups=nh, bias=False)\n",
    "    \n",
    "\n",
    "        #3rd layer\n",
    "        self.sqex = eSqueeze(nh, squeeze_ratio) if squeeze_ratio >0 else Identity\n",
    "\n",
    "        #4th layer - no relu\n",
    "        self.projection = econv_layer(nh, nf, ks=1, stride=1, bias=False, act=False)\n",
    "        \n",
    "\n",
    "        self.skip = skip and (stride==1) and (ni==nf)\n",
    "        if self.skip:\n",
    "            self.dropconnect = partial(drop_connect,p=drop_connect_prob, training=self.training)\n",
    "        else:\n",
    "            self.dropconnect= Identity\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        expand = self.expansion(inputs)\n",
    "        dwise = self.depthwise(expand)\n",
    "        se = self.sqex(dwise)\n",
    "        x = self.projection(se)\n",
    "        if self.skip:\n",
    "            x = x+ self.dropconnect(inputs)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_filters(filters, depth_multi, divisor=8, min_depth=None):\n",
    "    \n",
    "    \"\"\"Round number of filters based on depth multiplier.\n",
    "    see: https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/efficientnet_model.py\"\"\"\n",
    "    \n",
    "    orig_f = filters\n",
    "    \n",
    "    if not depth_multi:\n",
    "        return filters\n",
    "\n",
    "    filters = [f*depth_multi for f in filters]\n",
    "    min_depth = min_depth or divisor\n",
    "    new_filters = [max(min_depth, int(f + divisor / 2) // divisor * divisor) for f in filters]\n",
    "    # prevent rounding by more than 10%\n",
    "    new_filters = [new_filters[i] + (new_filters[i] < 0.9 * filters[i])* divisor for i in range(len(new_filters))]\n",
    "    new_filters = [int(f) for f in new_filters]\n",
    "    #print('round_filter input={} output={}'.format(orig_f, new_filters))\n",
    "    return new_filters\n",
    "\n",
    "\n",
    "def round_repeats(repeats, global_params):\n",
    "    \n",
    "    \"\"\"Round number of filters based on depth multiplier.\"\"\"\n",
    "    multiplier = global_params.depth_coefficient\n",
    "    if not multiplier:\n",
    "        return repeats\n",
    "    return int(math.ceil(multiplier * repeats))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_fn = Swish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Identity(x):\n",
    "    return x  #where is nn.Identity()?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = [32,16,24,40,80,112,192,320,1280]  #9 count\n",
    "repeat = [1,2,2,3,3,4,1]\n",
    "ks = [3,3,5,3,5,5,3]\n",
    "stride = [1,2,2,2,1,2,1]\n",
    "expand = [1,6,6,6,6,6,6]\n",
    "se = 0.25\n",
    "do = 0.2\n",
    "dc=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class effNet(nn.Sequential):\n",
    "    def __init__(self, \n",
    "                 channels = [32,16,24,40,80,112,192,320,1280], \n",
    "                 repeat=[1,2,2,3,3,4,1], \n",
    "                 ks=[3,3,5,3,5,5,3], \n",
    "                 stride = [1,2,2,2,1,2,1], \n",
    "                 expand = [1,6,6,6,6,6,6], \n",
    "                 width_multi=1.0, \n",
    "                 depth_multi=1.0, \n",
    "                 squeeze = .25, \n",
    "                 drop_connect_prob = .2,\n",
    "                 dropout_prob= .2, \n",
    "                 c_in=3, \n",
    "                 c_out=10):\n",
    "\n",
    "        \n",
    "        repeat = [int(math.ceil(r*depth_multi)) for r in repeat]\n",
    "        print(repeat)\n",
    "        \n",
    "        channels = round_filters(channels, width_multi)\n",
    "        print(channels)\n",
    "        \n",
    "        stem = [econv_layer(c_in, channels[0], ks=3 ,stride=2)] \n",
    "        print(stem)\n",
    "\n",
    "        blocks = []\n",
    "        #The first block needs to take care of stride and filter size increase.\n",
    "\n",
    "        for i in range(len(repeat)):\n",
    "            blocks+= [eMBConvBlock(channels[i], channels[i+1], expand[i], ks=ks[i], \n",
    "                                   stride=stride[i], squeeze_ratio = squeeze, drop_connect_prob=drop_connect_prob)]\n",
    "            \n",
    "            blocks+= [eMBConvBlock(channels[i+1], channels[i+1], expand[i], ks=ks[i], \n",
    "                                   stride=1, squeeze_ratio = squeeze, drop_connect_prob=drop_connect_prob)] *(repeat[i]-1)\n",
    "\n",
    "        dropout = nn.Dropout(p=dropout_prob) if dropout_prob else Identity()\n",
    "\n",
    "        head = [conv_layer(channels[-2], channels[-1], ks=1 ,stride=1), \n",
    "                nn.AdaptiveAvgPool2d(1), Flatten(), dropout, \n",
    "                nn.Linear(channels[-1], c_out)]\n",
    "\n",
    "\n",
    "        super().__init__(*stem, *blocks, *head)\n",
    "                      \n",
    "        init_cnn(self)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 2, 3, 3, 4, 1]\n",
      "[32, 16, 24, 40, 80, 112, 192, 320, 1280]\n",
      "[Sequential(\n",
      "  (0): Conv2dSamePadding(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "  (1): BatchNorm2d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): Swish()\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "arch = effNet(c_out=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "effNet(\n",
      "  (0): Sequential(\n",
      "    (0): Conv2dSamePadding(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "    (1): BatchNorm2d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): Swish()\n",
      "  )\n",
      "  (1): eMBConvBlock(\n",
      "    (depthwise): Sequential(\n",
      "      (0): Conv2dSamePadding(32, 32, kernel_size=(3, 3), stride=(1, 1), groups=32, bias=False)\n",
      "      (1): BatchNorm2d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Swish()\n",
      "    )\n",
      "    (sqex): eSqueeze(\n",
      "      (layers): Sequential(\n",
      "        (0): AdaptiveAvgPool2d(output_size=1)\n",
      "        (1): Conv2dSamePadding(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (2): Swish()\n",
      "        (3): Conv2dSamePadding(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "    (projection): Sequential(\n",
      "      (0): Conv2dSamePadding(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (2): eMBConvBlock(\n",
      "    (expansion): Sequential(\n",
      "      (0): Conv2dSamePadding(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Swish()\n",
      "    )\n",
      "    (depthwise): Sequential(\n",
      "      (0): Conv2dSamePadding(96, 96, kernel_size=(3, 3), stride=(2, 2), groups=96, bias=False)\n",
      "      (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Swish()\n",
      "    )\n",
      "    (sqex): eSqueeze(\n",
      "      (layers): Sequential(\n",
      "        (0): AdaptiveAvgPool2d(output_size=1)\n",
      "        (1): Conv2dSamePadding(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (2): Swish()\n",
      "        (3): Conv2dSamePadding(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "    (projection): Sequential(\n",
      "      (0): Conv2dSamePadding(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (3): eMBConvBlock(\n",
      "    (expansion): Sequential(\n",
      "      (0): Conv2dSamePadding(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Swish()\n",
      "    )\n",
      "    (depthwise): Sequential(\n",
      "      (0): Conv2dSamePadding(144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False)\n",
      "      (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Swish()\n",
      "    )\n",
      "    (sqex): eSqueeze(\n",
      "      (layers): Sequential(\n",
      "        (0): AdaptiveAvgPool2d(output_size=1)\n",
      "        (1): Conv2dSamePadding(144, 36, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (2): Swish()\n",
      "        (3): Conv2dSamePadding(36, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "    (projection): Sequential(\n",
      "      (0): Conv2dSamePadding(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (4): eMBConvBlock(\n",
      "    (expansion): Sequential(\n",
      "      (0): Conv2dSamePadding(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Swish()\n",
      "    )\n",
      "    (depthwise): Sequential(\n",
      "      (0): Conv2dSamePadding(144, 144, kernel_size=(5, 5), stride=(2, 2), groups=144, bias=False)\n",
      "      (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Swish()\n",
      "    )\n",
      "    (sqex): eSqueeze(\n",
      "      (layers): Sequential(\n",
      "        (0): AdaptiveAvgPool2d(output_size=1)\n",
      "        (1): Conv2dSamePadding(144, 36, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (2): Swish()\n",
      "        (3): Conv2dSamePadding(36, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "    (projection): Sequential(\n",
      "      (0): Conv2dSamePadding(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (5): eMBConvBlock(\n",
      "    (expansion): Sequential(\n",
      "      (0): Conv2dSamePadding(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Swish()\n",
      "    )\n",
      "    (depthwise): Sequential(\n",
      "      (0): Conv2dSamePadding(240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False)\n",
      "      (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Swish()\n",
      "    )\n",
      "    (sqex): eSqueeze(\n",
      "      (layers): Sequential(\n",
      "        (0): AdaptiveAvgPool2d(output_size=1)\n",
      "        (1): Conv2dSamePadding(240, 60, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (2): Swish()\n",
      "        (3): Conv2dSamePadding(60, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "    (projection): Sequential(\n",
      "      (0): Conv2dSamePadding(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (6): eMBConvBlock(\n",
      "    (expansion): Sequential(\n",
      "      (0): Conv2dSamePadding(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Swish()\n",
      "    )\n",
      "    (depthwise): Sequential(\n",
      "      (0): Conv2dSamePadding(240, 240, kernel_size=(3, 3), stride=(2, 2), groups=240, bias=False)\n",
      "      (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Swish()\n",
      "    )\n",
      "    (sqex): eSqueeze(\n",
      "      (layers): Sequential(\n",
      "        (0): AdaptiveAvgPool2d(output_size=1)\n",
      "        (1): Conv2dSamePadding(240, 60, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (2): Swish()\n",
      "        (3): Conv2dSamePadding(60, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "    (projection): Sequential(\n",
      "      (0): Conv2dSamePadding(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (7): eMBConvBlock(\n",
      "    (expansion): Sequential(\n",
      "      (0): Conv2dSamePadding(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Swish()\n",
      "    )\n",
      "    (depthwise): Sequential(\n",
      "      (0): Conv2dSamePadding(480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False)\n",
      "      (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Swish()\n",
      "    )\n",
      "    (sqex): eSqueeze(\n",
      "      (layers): Sequential(\n",
      "        (0): AdaptiveAvgPool2d(output_size=1)\n",
      "        (1): Conv2dSamePadding(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (2): Swish()\n",
      "        (3): Conv2dSamePadding(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "    (projection): Sequential(\n",
      "      (0): Conv2dSamePadding(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (8): eMBConvBlock(\n",
      "    (expansion): Sequential(\n",
      "      (0): Conv2dSamePadding(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Swish()\n",
      "    )\n",
      "    (depthwise): Sequential(\n",
      "      (0): Conv2dSamePadding(480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False)\n",
      "      (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Swish()\n",
      "    )\n",
      "    (sqex): eSqueeze(\n",
      "      (layers): Sequential(\n",
      "        (0): AdaptiveAvgPool2d(output_size=1)\n",
      "        (1): Conv2dSamePadding(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (2): Swish()\n",
      "        (3): Conv2dSamePadding(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "    (projection): Sequential(\n",
      "      (0): Conv2dSamePadding(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (9): eMBConvBlock(\n",
      "    (expansion): Sequential(\n",
      "      (0): Conv2dSamePadding(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Swish()\n",
      "    )\n",
      "    (depthwise): Sequential(\n",
      "      (0): Conv2dSamePadding(480, 480, kernel_size=(5, 5), stride=(1, 1), groups=480, bias=False)\n",
      "      (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Swish()\n",
      "    )\n",
      "    (sqex): eSqueeze(\n",
      "      (layers): Sequential(\n",
      "        (0): AdaptiveAvgPool2d(output_size=1)\n",
      "        (1): Conv2dSamePadding(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (2): Swish()\n",
      "        (3): Conv2dSamePadding(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "    (projection): Sequential(\n",
      "      (0): Conv2dSamePadding(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (10): eMBConvBlock(\n",
      "    (expansion): Sequential(\n",
      "      (0): Conv2dSamePadding(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Swish()\n",
      "    )\n",
      "    (depthwise): Sequential(\n",
      "      (0): Conv2dSamePadding(672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False)\n",
      "      (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Swish()\n",
      "    )\n",
      "    (sqex): eSqueeze(\n",
      "      (layers): Sequential(\n",
      "        (0): AdaptiveAvgPool2d(output_size=1)\n",
      "        (1): Conv2dSamePadding(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (2): Swish()\n",
      "        (3): Conv2dSamePadding(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "    (projection): Sequential(\n",
      "      (0): Conv2dSamePadding(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (11): eMBConvBlock(\n",
      "    (expansion): Sequential(\n",
      "      (0): Conv2dSamePadding(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Swish()\n",
      "    )\n",
      "    (depthwise): Sequential(\n",
      "      (0): Conv2dSamePadding(672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False)\n",
      "      (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Swish()\n",
      "    )\n",
      "    (sqex): eSqueeze(\n",
      "      (layers): Sequential(\n",
      "        (0): AdaptiveAvgPool2d(output_size=1)\n",
      "        (1): Conv2dSamePadding(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (2): Swish()\n",
      "        (3): Conv2dSamePadding(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "    (projection): Sequential(\n",
      "      (0): Conv2dSamePadding(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (12): eMBConvBlock(\n",
      "    (expansion): Sequential(\n",
      "      (0): Conv2dSamePadding(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Swish()\n",
      "    )\n",
      "    (depthwise): Sequential(\n",
      "      (0): Conv2dSamePadding(672, 672, kernel_size=(5, 5), stride=(2, 2), groups=672, bias=False)\n",
      "      (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Swish()\n",
      "    )\n",
      "    (sqex): eSqueeze(\n",
      "      (layers): Sequential(\n",
      "        (0): AdaptiveAvgPool2d(output_size=1)\n",
      "        (1): Conv2dSamePadding(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (2): Swish()\n",
      "        (3): Conv2dSamePadding(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "    (projection): Sequential(\n",
      "      (0): Conv2dSamePadding(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(192, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (13): eMBConvBlock(\n",
      "    (expansion): Sequential(\n",
      "      (0): Conv2dSamePadding(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1152, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Swish()\n",
      "    )\n",
      "    (depthwise): Sequential(\n",
      "      (0): Conv2dSamePadding(1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False)\n",
      "      (1): BatchNorm2d(1152, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Swish()\n",
      "    )\n",
      "    (sqex): eSqueeze(\n",
      "      (layers): Sequential(\n",
      "        (0): AdaptiveAvgPool2d(output_size=1)\n",
      "        (1): Conv2dSamePadding(1152, 288, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (2): Swish()\n",
      "        (3): Conv2dSamePadding(288, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "    (projection): Sequential(\n",
      "      (0): Conv2dSamePadding(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(192, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (14): eMBConvBlock(\n",
      "    (expansion): Sequential(\n",
      "      (0): Conv2dSamePadding(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1152, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Swish()\n",
      "    )\n",
      "    (depthwise): Sequential(\n",
      "      (0): Conv2dSamePadding(1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False)\n",
      "      (1): BatchNorm2d(1152, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Swish()\n",
      "    )\n",
      "    (sqex): eSqueeze(\n",
      "      (layers): Sequential(\n",
      "        (0): AdaptiveAvgPool2d(output_size=1)\n",
      "        (1): Conv2dSamePadding(1152, 288, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (2): Swish()\n",
      "        (3): Conv2dSamePadding(288, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "    (projection): Sequential(\n",
      "      (0): Conv2dSamePadding(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(192, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (15): eMBConvBlock(\n",
      "    (expansion): Sequential(\n",
      "      (0): Conv2dSamePadding(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1152, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Swish()\n",
      "    )\n",
      "    (depthwise): Sequential(\n",
      "      (0): Conv2dSamePadding(1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False)\n",
      "      (1): BatchNorm2d(1152, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Swish()\n",
      "    )\n",
      "    (sqex): eSqueeze(\n",
      "      (layers): Sequential(\n",
      "        (0): AdaptiveAvgPool2d(output_size=1)\n",
      "        (1): Conv2dSamePadding(1152, 288, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (2): Swish()\n",
      "        (3): Conv2dSamePadding(288, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "    (projection): Sequential(\n",
      "      (0): Conv2dSamePadding(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(192, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (16): eMBConvBlock(\n",
      "    (expansion): Sequential(\n",
      "      (0): Conv2dSamePadding(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1152, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Swish()\n",
      "    )\n",
      "    (depthwise): Sequential(\n",
      "      (0): Conv2dSamePadding(1152, 1152, kernel_size=(3, 3), stride=(1, 1), groups=1152, bias=False)\n",
      "      (1): BatchNorm2d(1152, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Swish()\n",
      "    )\n",
      "    (sqex): eSqueeze(\n",
      "      (layers): Sequential(\n",
      "        (0): AdaptiveAvgPool2d(output_size=1)\n",
      "        (1): Conv2dSamePadding(1152, 288, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (2): Swish()\n",
      "        (3): Conv2dSamePadding(288, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (4): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "    (projection): Sequential(\n",
      "      (0): Conv2dSamePadding(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(320, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (17): Sequential(\n",
      "    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): GeneralRelu()\n",
      "    (2): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (18): AdaptiveAvgPool2d(output_size=1)\n",
      "  (19): Flatten()\n",
      "  (20): Dropout(p=0.2)\n",
      "  (21): Linear(in_features=1280, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(e0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "me = sys.modules[__name__]\n",
    "channels = [32,16,24,40,80,112,192,320,1280]  #9 count\n",
    "repeat = [1,2,2,3,3,4,1]\n",
    "ks = [3,3,5,3,5,5,3]\n",
    "stride = [1,2,2,2,1,2,1]\n",
    "exp = [1,6,6,6,6,6,6]\n",
    "se = 0.25\n",
    "do = 0.2\n",
    "dc=0.2\n",
    "\n",
    "\n",
    "# base without multipliers and dropout\n",
    "setattr(me, 'effnet', partial(effNet, channels=channels, repeat=repeat, ks=ks, stride=stride, \n",
    "                                    expand=exp, se=se, drop_connect_rate=dc))\n",
    "\n",
    "# (number, width_coefficient, depth_coefficient, dropout_rate) \n",
    "for n, wm, dm, do in [\n",
    "    [ 0, 1.0, 1.0, 0.2],\n",
    "    [ 1, 1.0, 1.1, 0.2],\n",
    "    [ 2, 1.1, 1.2, 0.3],\n",
    "    [ 3, 1.2, 1.4, 0.3],\n",
    "    [ 4, 1.4, 1.8, 0.4],\n",
    "    [ 5, 1.6, 2.2, 0.4],\n",
    "    [ 6, 1.8, 2.6, 0.5],\n",
    "    [ 7, 2.0, 3.1, 0.5],\n",
    "]:\n",
    "    name = f'effNetB{n}'\n",
    "    setattr(me, name, partial(effnet, depth_multi=dm, width_multi=wm, dropout_rate=do))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imagenet(te) training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightRelu(nn.Module):\n",
    "    #.46 was found to shift the mean to 0 on a random distribution test\n",
    "    # maxv of 7.5 was from initial testing on MNIST.  \n",
    "    #Important - cut your learning rates in half with this...\n",
    "    \n",
    "    def __init__(self,sub=.2,maxv=None):\n",
    "        super().__init__()\n",
    "        self.sub=sub\n",
    "        self.maxv=maxv\n",
    "    \n",
    "    def forward(self,x):\n",
    "        #change to lisht\n",
    "        \n",
    "        x = x *torch.tanh(x)\n",
    "        \n",
    "        if self.sub is not None:\n",
    "            x.sub_(self.sub)\n",
    "        if self.maxv is not None: \n",
    "            x.clamp_max_(self.maxv)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = datasets.untar_data(datasets.URLs.IMAGENETTE_160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 128\n",
    "tfms = [make_rgb, RandomResizedCrop(128,scale=(0.35,1)), np_to_float, PilRandomFlip()]\n",
    "\n",
    "bs = 24\n",
    "\n",
    "il = ImageList.from_files(path, tfms=tfms)\n",
    "sd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name='val'))\n",
    "ll = label_by_func(sd, parent_labeler, proc_y=CategoryProcessor())\n",
    "\n",
    "ll.valid.x.tfms = [make_rgb, CenterCrop(size), np_to_float]\n",
    "\n",
    "data = ll.to_databunch(bs, c_in=3, c_out=10, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def noop(x): return x\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x): return x.view(x.size(0), -1)\n",
    "\n",
    "def conv(ni, nf, ks=3, stride=1, bias=False):\n",
    "    return nn.Conv2d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2, bias=bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noop(x): return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self,x): return x.view(x.size(0),-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#act_fn = LightRelu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralRelu(nn.Module):\n",
    "    def __init__(self, leak=.2, sub=.3, maxv=12):\n",
    "        super().__init__()\n",
    "        self.leak,self.sub,self.maxv = leak,sub,maxv\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = F.leaky_relu(x,self.leak) if self.leak is not None else F.relu(x)\n",
    "        if self.sub is not None: x.sub_(self.sub)\n",
    "        if self.maxv is not None: x.clamp_max_(self.maxv)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FTSwish2(nn.Module):\n",
    "    def __init__(self, threshold=-.25):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold #,self.sub,self.maxv = threshold,sub,maxv\n",
    "\n",
    "    def forward(self, x): \n",
    "        print(x)\n",
    "        #if x > 0:\n",
    "        x = (x*torch.sigmoid(x)) + threshold\n",
    "       # else:\n",
    "       #     x = threshold\n",
    "            \n",
    "        #if self.sub is not None: x.sub_(self.sub)\n",
    "       # if self.maxv is not None: x.clamp_max_(self.maxv)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#act_fn = LightRelu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#act_fn = nn.ReLU(inplace=True)\n",
    "\n",
    "def init_cnn(m):\n",
    "    if getattr(m, 'bias', None) is not None: nn.init.constant_(m.bias, 0)\n",
    "    if isinstance(m, (nn.Conv2d,nn.Linear)): nn.init.kaiming_normal_(m.weight)\n",
    "    for l in m.children(): init_cnn(l)\n",
    "\n",
    "def conv_layer(ni, nf, ks=3, stride=1, zero_bn=False, act=True):\n",
    "    bn = nn.BatchNorm2d(nf)\n",
    "    nn.init.constant_(bn.weight, 0. if zero_bn else 1.)\n",
    "    layers = [conv(ni, nf, ks, stride=stride), bn]\n",
    "    if act: layers.append(act_fn)\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#act_fn = nn.ReLU(inplace=True)\n",
    "\n",
    "def init_cnn2(m):\n",
    "    if getattr(m, 'bias',None) is not None:  nn.init.constant_(m.bias,0)\n",
    "    if isinstance(m, (nn.Conv2d, nn.Linear)): nn.init.kaiming_normal_(m.weight)\n",
    "    for l in m.children(): init_cnn(l)\n",
    "        \n",
    "def conv_layer2(ni, nf, ks=3, stride = 1, zero_bn=False, act=True):\n",
    "    bn = nn.BatchNorm2d(nf)\n",
    "    nn.init.constant_(bn.weight,0. if zero_bn else 1.)\n",
    "    layers = [conv(ni,nf,ks,stride=stride),bn]\n",
    "    if act: layers.append(act_fn)\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, expansion, ni, nh, stride=1):\n",
    "        super().__init__()\n",
    "        nf,ni = nh*expansion,ni*expansion\n",
    "        layers  = [conv_layer(ni, nh, 1)]\n",
    "        layers += [\n",
    "            conv_layer(nh, nf, 3, stride=stride, zero_bn=True, act=False)\n",
    "        ] if expansion==1 else [\n",
    "            conv_layer(nh, nh, 3, stride=stride),\n",
    "            conv_layer(nh, nf, 1, zero_bn=True, act=False)\n",
    "        ]\n",
    "        self.convs = nn.Sequential(*layers)\n",
    "        self.idconv = noop if ni==nf else conv_layer(ni, nf, 1, act=False)\n",
    "        self.pool = noop if stride==1 else nn.AvgPool2d(2)\n",
    "\n",
    "    def forward(self, x): return act_fn(self.convs(x) + self.idconv(self.pool(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock2(nn.Module):\n",
    "    def __init__(self, expansion, ni,nh, stride=1):\n",
    "        super().__init__()\n",
    "        nf, ni = nh*expansion, ni*expansion  #number of filters/fields, number of inputs\n",
    "        layers = [conv_layer(ni,nh,1)]  #base layer\n",
    "        layers += [\n",
    "            conv_layer(nh,nh,3,stride=stride, zero_bn=True, act=False) #add new conv layer if expansion =1 else\n",
    "        ] if expansion==1 else [\n",
    "            conv_layer(nh,nh,3,stride=stride),\n",
    "            conv_layer(nh,nf,1, zero_bn=True, act=False) #add two conv layerss\n",
    "        ]\n",
    "        self.convs = nn.Sequential(*layers)  #wrap it in a sequential\n",
    "        self.idconv = noop if ni==nf else conv_layer(ni,nf,1,act=False)  # add id layer\n",
    "        self.pool = noop if stride==1 else nn.AvgPool2d(2) # add pool layer\n",
    "        \n",
    "    def forward(self,x): return act_fn(self.convs(x)+ self.idconv(self.pool(x))) #wrap block in relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class XResNet(nn.Sequential):\n",
    "    @classmethod\n",
    "    def create(cls, expansion, layers, c_in=3, c_out=1000):\n",
    "        nfs = [c_in, (c_in+1)*8, 64, 64]\n",
    "        stem = [conv_layer(nfs[i], nfs[i+1], stride=2 if i==0 else 1)\n",
    "            for i in range(3)]\n",
    "\n",
    "        nfs = [64//expansion,64,128,256,512]\n",
    "        res_layers = [cls._make_layer(expansion, nfs[i], nfs[i+1],\n",
    "                                      n_blocks=l, stride=1 if i==0 else 2)\n",
    "                  for i,l in enumerate(layers)]\n",
    "        res = cls(\n",
    "            *stem,\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            *res_layers,\n",
    "            nn.AdaptiveAvgPool2d(1), Flatten(),\n",
    "            nn.Linear(nfs[-1]*expansion, c_out),\n",
    "        )\n",
    "        init_cnn(res)\n",
    "        return res\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_layer(expansion, ni, nf, n_blocks, stride):\n",
    "        return nn.Sequential(\n",
    "            *[ResBlock(expansion, ni if i==0 else nf, nf, stride if i==0 else 1)\n",
    "              for i in range(n_blocks)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def xresnet18 (**kwargs): return XResNet.create(1, [2, 2, 2, 2], **kwargs)\n",
    "def xresnet34 (**kwargs): return XResNet.create(1, [3, 4, 6, 3], **kwargs)\n",
    "def xresnet50 (**kwargs): return XResNet.create(4, [3, 4, 6, 3], **kwargs)\n",
    "def xresnet101(**kwargs): return XResNet.create(4, [3, 4, 23, 3], **kwargs)\n",
    "def xresnet152(**kwargs): return XResNet.create(4, [3, 8, 36, 3], **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbfs = [partial(AvgStatsCallback,accuracy), ProgressCallback, CudaCallback,\n",
    "        partial(BatchTransformXCallback, norm_imagenette),\n",
    "#         partial(MixUp, alpha=0.2)\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = LabelSmoothingCrossEntropy()\n",
    "arch = partial(effNet, c_out=10)\n",
    "opt_func = adam_opt(mom=0.9, mom_sqr=0.99, eps=1e-6, wd=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_batch(dl, learn):\n",
    "    learn.xb,learn.yb = next(iter(dl))\n",
    "    learn.do_begin_fit(0)\n",
    "    learn('begin_batch')\n",
    "    learn('after_fit')\n",
    "    return learn.xb,learn.yb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to replace the old `model_summary` since it used to take a `Runner`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def model_summary(model, find_all=False, print_mod=False):\n",
    "    xb,yb = get_batch(data.valid_dl, learn)\n",
    "    mods = find_modules(model, is_lin_layer) if find_all else model.children()\n",
    "    f = lambda hook,mod,inp,out: print(f\"====\\n{mod}\\n\" if print_mod else \"\", out.shape)\n",
    "    with Hooks(mods, f) as hooks: learn.model(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 2, 3, 3, 4, 1]\n",
      "[32, 16, 24, 40, 80, 112, 192, 320, 1280]\n",
      "[Sequential(\n",
      "  (0): Conv2dSamePadding(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "  (1): BatchNorm2d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): Swish()\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "learn = Learner(arch(), data, loss_func, lr=1, cb_funcs=cbfs, opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch.Size([48, 32, 64, 64])\n",
      " torch.Size([48, 16, 64, 64])\n",
      " torch.Size([48, 24, 32, 32])\n",
      " torch.Size([48, 24, 32, 32])\n",
      " torch.Size([48, 40, 16, 16])\n",
      " torch.Size([48, 40, 16, 16])\n",
      " torch.Size([48, 80, 8, 8])\n",
      " torch.Size([48, 80, 8, 8])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 2.00 GiB total capacity; 1.30 GiB already allocated; 11.43 MiB free; 26.39 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-151-c855843b45a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_mod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-139-b9b7b07b89f9>\u001b[0m in \u001b[0;36mmodel_summary\u001b[1;34m(model, find_all, print_mod)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mmods\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_modules\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_lin_layer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mfind_all\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"====\\n{mod}\\n\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mprint_mod\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mHooks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmods\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3Sept\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3Sept\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3Sept\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-127-7dfbe5699b03>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mexpand\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpansion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0mdwise\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdepthwise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdwise\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3Sept\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3Sept\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3Sept\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3Sept\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m             exponential_average_factor, self.eps)\n\u001b[0m\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3Sept\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   1667\u001b[0m     return torch.batch_norm(\n\u001b[0;32m   1668\u001b[0m         \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1669\u001b[1;33m         \u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1670\u001b[0m     )\n\u001b[0;32m   1671\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 2.00 GiB total capacity; 1.30 GiB already allocated; 11.43 MiB free; 26.39 MiB cached)"
     ]
    }
   ],
   "source": [
    "learn.model = learn.model.cuda()\n",
    "model_summary(learn.model, print_mod=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arch = partial(xresnet50, c_out=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(arch(), data, loss_func, lr=1, cb_funcs=cbfs, opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(1, cbs=[LR_Find(), Recorder()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.recorder.plot(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.recorder.plot_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def create_phases(phases):\n",
    "    phases = listify(phases)\n",
    "    return phases + [1-sum(phases)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3, 0.7]\n",
      "[0.3, 0.2, 0.5]\n"
     ]
    }
   ],
   "source": [
    "print(create_phases(0.3))\n",
    "print(create_phases([0.3,0.2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "pct_start = 0.5\n",
    "phases = create_phases(pct_start)\n",
    "sched_lr  = combine_scheds(phases, cos_1cycle_anneal(lr/10., lr, lr/1e5))\n",
    "sched_mom = combine_scheds(phases, cos_1cycle_anneal(0.95,0.85, 0.95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-15\n"
     ]
    }
   ],
   "source": [
    "print(1e-15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbsched = [\n",
    "    ParamScheduler('lr', sched_lr),\n",
    "    ParamScheduler('mom', sched_mom)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn = Learner(arch(), data, loss_func, lr=lr, cb_funcs=cbfs, opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.fit(1, cbs=cbsched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cnn_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def cnn_learner(arch, data, loss_func, opt_func, c_in=None, c_out=None,\n",
    "                lr=1e-1, cuda=True, norm=None, progress=True, mixup=0, xtra_cb=None, **kwargs):\n",
    "    cbfs = [partial(AvgStatsCallback,accuracy)]+listify(xtra_cb)\n",
    "    if progress: cbfs.append(ProgressCallback)\n",
    "    if cuda:     cbfs.append(CudaCallback)\n",
    "    if norm:     cbfs.append(partial(BatchTransformXCallback, norm))\n",
    "    if mixup:    cbfs.append(partial(MixUp, mixup))\n",
    "    arch_args = {}\n",
    "    if not c_in : c_in  = data.c_in\n",
    "    if not c_out: c_out = data.c_out\n",
    "    if c_in:  arch_args['c_in' ]=c_in\n",
    "    if c_out: arch_args['c_out']=c_out\n",
    "    return Learner(arch(**arch_args), data, loss_func, opt_func=opt_func, lr=lr, cb_funcs=cbfs, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch.nn.functional as F  (uncomment if needed)\n",
    "\n",
    "class FTSwish(nn.Module):\n",
    "    def __init__(self, threshold=-.25, mean_shift=-.1):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "        self.mean_shift = mean_shift\n",
    "        #warning - does not handle multi-gpu case below\n",
    "        #self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "\n",
    "    def forward(self, x): \n",
    "        \n",
    "        x = F.relu(x) * torch.sigmoid(x) + self.threshold\n",
    "        \n",
    "        #note on above: (\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
    "        \n",
    "        \n",
    "        #FTSwish+ for positive values\n",
    "        #pos_value = (x*torch.sigmoid(x)) + self.threshold\n",
    "        \n",
    "        #FTSwish+ for negative values\n",
    "        #tval = torch.tensor([self.threshold],device=self.device)\n",
    "        \n",
    "        #apply to x tensor based on positive or negative value\n",
    "        #x = torch.where(x>=0, pos_value, tval)\n",
    "        \n",
    "        \n",
    "        #apply mean shift to drive mean to 0. -.1 was tested as optimal for kaiming init\n",
    "        if self.mean_shift is not None:\n",
    "            x.sub_(self.mean_shift)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch.nn.functional as F  (uncomment if needed,but you likely already have it)\n",
    "\n",
    "class FTSwishPlus(nn.Module):\n",
    "    def __init__(self, threshold=-.25, mean_shift=-.1):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "        self.mean_shift = mean_shift\n",
    "\n",
    "    def forward(self, x): \n",
    "        \n",
    "        x = F.relu(x) * torch.sigmoid(x) + self.threshold        \n",
    "        #note on above - why not F.sigmoid?: \n",
    "        #PyTorch docs - (\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
    "        \n",
    "        #apply mean shift to drive mean to 0. -.1 was tested as optimal for kaiming init\n",
    "        if self.mean_shift is not None:\n",
    "            x.sub_(self.mean_shift)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRelu(nn.Module):\n",
    "    def __init__(self, threshold= - .25, mean_shift=-.03):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "        self.mean_shift = mean_shift\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = F.relu(x)+self.threshold\n",
    "        \n",
    "        if self.mean_shift is not None:\n",
    "            x.sub_(self.mean_shift)\n",
    "            \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_fn = ReluT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(FTSwish.forward(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'c_in'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-147-a832f5664e0a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlearn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcnn_learner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0march\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnorm_imagenette\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-145-4c6f853b981d>\u001b[0m in \u001b[0;36mcnn_learner\u001b[1;34m(arch, data, loss_func, opt_func, c_in, c_out, lr, cuda, norm, progress, mixup, xtra_cb, **kwargs)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mc_in\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[0march_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'c_in'\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mc_in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mc_out\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0march_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'c_out'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mc_out\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mLearner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0march\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0march_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_func\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopt_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcb_funcs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcbfs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3Sept\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'c_in'"
     ]
    }
   ],
   "source": [
    "learn = cnn_learner(arch, data, loss_func, opt_func, norm=norm_imagenette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.708927</td>\n",
       "      <td>0.475260</td>\n",
       "      <td>1.455793</td>\n",
       "      <td>0.612000</td>\n",
       "      <td>11:57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.380989</td>\n",
       "      <td>0.635567</td>\n",
       "      <td>1.319252</td>\n",
       "      <td>0.676000</td>\n",
       "      <td>09:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.329379</td>\n",
       "      <td>0.660540</td>\n",
       "      <td>1.855722</td>\n",
       "      <td>0.558000</td>\n",
       "      <td>09:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.308054</td>\n",
       "      <td>0.666822</td>\n",
       "      <td>1.320109</td>\n",
       "      <td>0.674000</td>\n",
       "      <td>10:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.275966</td>\n",
       "      <td>0.684970</td>\n",
       "      <td>1.510024</td>\n",
       "      <td>0.626000</td>\n",
       "      <td>09:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.232606</td>\n",
       "      <td>0.706685</td>\n",
       "      <td>1.111155</td>\n",
       "      <td>0.756000</td>\n",
       "      <td>10:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.167539</td>\n",
       "      <td>0.733209</td>\n",
       "      <td>1.037894</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>11:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.093275</td>\n",
       "      <td>0.758803</td>\n",
       "      <td>1.066288</td>\n",
       "      <td>0.788000</td>\n",
       "      <td>12:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.015583</td>\n",
       "      <td>0.792772</td>\n",
       "      <td>0.855244</td>\n",
       "      <td>0.866000</td>\n",
       "      <td>09:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.935219</td>\n",
       "      <td>0.826198</td>\n",
       "      <td>0.846282</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>17:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.862420</td>\n",
       "      <td>0.861951</td>\n",
       "      <td>0.790140</td>\n",
       "      <td>0.896000</td>\n",
       "      <td>09:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.840959</td>\n",
       "      <td>0.868233</td>\n",
       "      <td>0.781657</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>14:12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(1, cbsched) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.856373</td>\n",
       "      <td>0.861176</td>\n",
       "      <td>0.874657</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>13:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.950700</td>\n",
       "      <td>0.820149</td>\n",
       "      <td>0.956492</td>\n",
       "      <td>0.824000</td>\n",
       "      <td>16:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.022369</td>\n",
       "      <td>0.792229</td>\n",
       "      <td>1.068875</td>\n",
       "      <td>0.768000</td>\n",
       "      <td>15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.942604</td>\n",
       "      <td>0.827749</td>\n",
       "      <td>0.840393</td>\n",
       "      <td>0.876000</td>\n",
       "      <td>07:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.836331</td>\n",
       "      <td>0.873740</td>\n",
       "      <td>0.750392</td>\n",
       "      <td>0.904000</td>\n",
       "      <td>05:29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(5,cbsched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imagenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see all this put together in the fastai [imagenet training script](https://github.com/fastai/fastai/blob/master/examples/train_imagenet.py). It's the same as what we've seen so far, except it also handles multi-GPU training. So how well does this work?\n",
    "\n",
    "We trained for 60 epochs, and got an error of 5.9%, compared to the official PyTorch resnet which gets 7.5% error in 90 epochs! Our xresnet 50 training even surpasses standard resnet 152, which trains for 50% more epochs and has 3x as many layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "##!./notebook2script.py 11_train_imagenette.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
